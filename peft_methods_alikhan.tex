\documentclass[9pt]{beamer}

\input{StyleFile.tex}

\usepackage{wrapfig}

\usepackage{tikz}
\usepackage[main=russian,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}

\usepackage[backend=biber,style=numeric,urldate=long,maxbibnames=99,sorting=none]{biblatex}
\addbibresource{refs.bib}

\title[\textbf{PEFT}]{Parameter-Efficient Fine-Tuning} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[\textbf{Алихан Зиманов}]
{Алихан Зиманов} % Your name

\institute[\textbf{ПМИ ФКН НИУ ВШЭ}] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{Факультет компьютерных наук\\
НИУ ВШЭ % Your institution for the title page
}

\jointwork{feat. Илья Пахалко}
\conference{НИС, Москва, 2023}

%===== Uncomment the following if you wish to use references
%\usepackage[backend=bibtex,citestyle=authoryear-icomp,natbib,maxcitenames=1]{biblatex}
%\addbibresource{bibfile.bib}

% use this so appendices' page numbers do not count
\usepackage{appendixnumberbeamer}

\begin{document}

% Title page, navigation surpressed, no page number
{
\beamertemplatenavigationsymbolsempty
\begin{frame}[plain]
\titlepage
\end{frame}
}

% TOC, navigation surpressed, no page number
{
\beamertemplatenavigationsymbolsempty
\defbeamertemplate*{headline}{miniframes theme no subsection no content}
{ \begin{beamercolorbox}{section in head/foot}
    \vskip\headheight
  \end{beamercolorbox}}
\begin{frame}{Содежрание} 
\tableofcontents 
\end{frame} 
}

\addtocounter{framenumber}{-2}


\section{Введение}


\begin{frame}{Transfer Learning}

    \begin{definition}
        Применение опыта, полученного при решении одной задачи для решения новой задачи из той же области.
    \end{definition}

    \begin{example}[Классификация изображений]
        ResNet, обученный на ImageNet будет содержать в себе информацию о паттернах в картинках в целом, поэтому можно эту абстрактную информацию переиспользовать для решения других, более узких задач.
    \end{example}

    \begin{example}[Языковая модель]
        Эмбеддинги, обученные на колоссальных датасетах будут содержать в себе сложную информацию о структуре языка, поэтому их переиспользование для других задач NLP будет весьма удобным.
    \end{example}

\end{frame}


\begin{frame}{Подходы в NLP}

    \begin{block}{Pre-trained text representations (feature-based transfer)}
        Использование эбмеддингов, обученных на огромных датасетах для получения полезных признаковых описаний токенов.
    \end{block}

    \begin{block}{Fine-tuning}
        Использование уже обученной, но на другую задачу модели как инициализацию весов модели, решающей текущую задачу.
    \end{block}

    \begin{block}{Parameter-Efficient Fine-Tuning}
        Fine-tuning, обучающий как можно меньшее количество добавленных параметров или параметров исходной модели, при этом не теряющий точности решения задачи.
    \end{block}

\end{frame}

\begin{frame}{Техники обучения в NLP}

    \begin{block}{Multi-task Learning}
        Обучение модели на несколько задач одновременно. У модели имеются общие по всем задачам глубокие слои и специализированные под каждую задачу верхние слои.
    \end{block}

    \begin{block}{Continual Learning}
        Последовательное переобучение\footnote{не overfitting, а re-training} модели под каждую новую задачу.
    \end{block}

\end{frame}


\section{Adapter Tuning}

\begin{frame}{Adapter Tuning}

    Первая рассматриваемая техника PEFT это Adapter Tuning~\cite{adapter}. Хочется придумать такую технику, которая позволит решить следующие проблемы:
    
    \begin{itemize}
        \item Fine-tuning обучает все веса модели, что очень медленно и хочется обучать только малую часть весов;
        \item Multi-task learning подразумевает одновременное решение задач, что иногда бывает сложным, поэтому хочется уметь решать несколько задач, при этом не имея доступ ко всем задачам одновременно;
        \item Continual learning подразумевает дообучение имеющейся модели под новую задачу, при этом либо способность модели решать старую задачу утрачивается и сильно страдает качество, либо количество обучаемых параметров линейно возрастает с каждой следующей задачей.
    \end{itemize}

\end{frame}


\begin{frame}{Махания руками}

    Рассмотрим функцию (нейронную сеть) с параметрами $w$: $\phi_w(x)$.
    
    \begin{block}{Feature-based Transfer}
        Этот метод предлагает использовать композицию $\phi_w$ с новой функцией $\chi_v$, получая, таким образом, новую нейронную сеть $\chi_v(\phi_w(x))$. После этого обучаются только веса $v$, специфичные под текущую задачу.
    \end{block}

    \begin{block}{Fine-tuning}
        Этот метод предлагает дообучение имеющихся параметров $w$ под новую задачу, ограничивая компактность\footnote{компактная модель использует малое количество дополнительных параметров для решения новых задач} модели.
    \end{block}
    
\end{frame}


\begin{frame}{Махания руками 2.0}
    
    \begin{block}{Adapter tuning}
        Этот метод предлагает ввести новую функцию $\psi_{w, v}(x)$, где $w$ это в точности те же параметры, что и в $\phi_w$. Новые параметры $v$ инициализируются так, чтобы новая функция повторяла исходную: $\psi_{w, v_0}(x) \approx \phi_w(x)$. Во время обучения обновляются только веса $v$, а $w$ остаются неизменными. В идеале мы хотим строить функции $\psi$ такие, что $|v| \ll |w|$, чтобы размер модели практически не возрастал при добавлении новых параметров.
    \end{block}

    \begin{block}{Как это делать?}
        Для решения каждой новой задачи между слоями исходной модели будем добавлять новые, так называемые, adapter слои. В процессе обучения обновляться будут только веса этих добавленных слоев.
    \end{block}

\end{frame}


\begin{frame}{Adapter tuning для трансформеров}

    \begin{columns}
        \column{0.45\textwidth} \begin{figure}
            \caption{Слева блок трансформера, справа слой адаптера}
            \label{adapter:block}
            \includegraphics[scale=0.5]{images/adapter_1.jpg}
        \end{figure}
        \column{0.55\textwidth} \begin{block}{Transformer}
            В трансформере есть два главных типа слоёв: \textit{attention} слой и \textit{feedforward} слой. Слой \textit{adapter} добавляется сразу после каждой пары указанных слоев, но перед слоем нормализации и перед \textit{skip connection} слоем.
        \end{block} \begin{block}{Adapter}
            Слой адаптера по смыслу сначала проецирует $d$-мерные вектора признаков в меньшее $m$-мерное пространство, применяют нелинейность и проецируют вектора обратно в $d$-мерное пространство, добавив skip connection.
        \end{block}
    \end{columns}

\end{frame}


\begin{frame}{Махания руками 3.0}

    \begin{block}{Количество параметров}
        Заметим, что количество обучаемых параметров в одном adapter слое будет $2 m d + d + m$. Если брать $m \ll d$, мы можем легко ограничить количество добавляемых параметров для решения новой задачи. На практике можно добиться добавления всего $0.5 - 8\%$ параметров от исходной модели.
    \end{block}

    \begin{block}{Инициализация}
        Если инициализировать веса adapter слоя околонулевыми числами, то skip connection слой позволит новой модели имитировать исходную модель.
    \end{block}

    \begin{block}{Удобно?}
        Да\footnote{потому что новые задачи не портят качество на старых задачах в силу неизменения старых весов и количество новых параметров очень мало}.
    \end{block}
    
\end{frame}


\begin{frame}{Эксперименты}
    
    \begin{block}{Результаты}
        На GLUE бенчмарке adapter tuning позволяет достичь результата в диапазоне $0.4\%$ от fine-tuning всех параметров модели BERT, при этом adapter tuning добавляет и обучает всего $3\%$ от общего количества параметров модели.
    \end{block}

    \begin{figure}
        \begin{center}
            \includegraphics[scale=0.45]{images/adapter_2.jpg}
        \end{center}
    \end{figure}

\end{frame}


\section{Prefix-Tuning}

\begin{frame}{Prefix-Tuning}

    \begin{block}{Prefix-tuning}
        Вторая рассматриваемая техника PEFT это Prefix Tuning~\cite{prefix}. Эта техника пытается решить те же проблемы, что и adapter tuning, а именно обучаться на новые задачи не теряя способности решать старые, а также обучение минимального количества весов модели\footnote{оказывается у этого есть название --- lightweight fine-tunning}, чтобы для новых задач не требовалось хранить полные копии исходной модели.
    \end{block}

    \begin{block}{Задачи}
        Авторы статьи предлагают применить эту технику для задач типа table-to-text и summarization.

        \begin{itemize}
            \item Table-to-text --- генерация текстового описания объекта по структурированному табличному описанию. Например, из табличного представления $(\text{name}: \text{'Starbucks'}, \text{type}: \text{'coffee shop'})$ хотим получить текстовое описание: 'Starbucks serves coffee'.
            \item Summarization --- генерация краткого текстового описания более длинного исходного текста.
        \end{itemize}

    \end{block}

\end{frame}


\begin{frame}{Махания руками}

    \begin{block}{Контекст}
        В языковых моделях для генерации текста используются векторы контекста\footnote{вспоминаем рекуррентные нейронные сети}, призванные помогать модели понимать, что ей следует предсказывать дальше.
    \end{block}

    \begin{block}<2->{Гениальная идея}
        Можно ли придумать такой контекст в виде префикса, дописываемого перед каждыми входными данными, который будет указывать модели не просто генерируемый результат, а саму задачу, которая модель должна решать?

        \begin{solution}<3->
            Да!
        \end{solution}
    \end{block}

\end{frame}


\begin{frame}{Красивая картинка}

    \begin{figure}
        \caption{Сравнение количества обучаемых параметров в полном fine-tuning и prefix-tuning}
        \begin{center}
            \includegraphics[scale=0.5]{images/prefix_1.jpg}
        \end{center}
    \end{figure}

\end{frame}


\begin{frame}{Ещё одна красивая картинка}

    \begin{figure}
        \caption{Куда пихать перфикс}
        \begin{center}
            \includegraphics[scale=0.35]{images/prefix_4.jpg}
        \end{center}
    \end{figure}

\end{frame}


\begin{frame}{Формулки}

    \begin{block}{Авторегрессионая языковая модель}
        Предположим, что у нас есть модель $p_{\phi}(y | x)$, основанная на трансформере и параметризованная от $\phi$. Пусть $z = [x; y]$ --- конкатенация входных и выходных данных одного примера. Так как это авторегрессионая модель, то можем ввести вектора активации $h_i = [h_i^{(1)}; \ldots ; h_i^{(n)}]$, где $i$ отвечает за момент времени $i$ (то есть когда мы будем пихать в модель $i$-ый токен из $z$), а верхний индекс отвечает за номер слоя трансформера. Тогда трансформер будет вычислять эти вектора как: \[ h_i = LM_{\phi}(z_i, h_{<i}), \] а распределение вероятностей для предсказания следующего токена вычисляется как софтмакс от классификатора после последнего слоя $h_i^{(n)}$.
    \end{block}

    \begin{block}{Encoder-Decoder}
        Эта архитектура отличается только тем, что вектора $h_i$ для входных данных считаются в энкодере, а для выходных данных считаются в декодере.
    \end{block}

\end{frame}


\begin{frame}{Метод}
    
    \begin{block}{База}
        В авторегрессионной модели допихиваем префикс как $z = [\text{PREFIX}; x; y]$, а в encoder-decoder $z = [\text{PREFIX}; x; \text{PREFIX'}; y]$. Пусть $P_{idx}$ --- последовательность индексов добавленных в $z$ префиксов. Тогда мы хотим обучать матрицу $P_{\theta}$, хранящую вектора активации для префиксов: \[ h_i = \begin{cases}
            P_{\theta}[i, :], & \text{if } i \in P_{idx}, \\
            LM_{\phi}(z_i, h_{<i}), & \text{otherwise}.
        \end{cases} \]
    \end{block}

    \begin{block}{Фундамент}
        Учить матрицу $P_{\theta}$ в лоб выходит неэффективно, поэтому можно представить её в виде $P_{\theta}[i, :] = MLP_{\theta}(P_{\theta}'[i, :])$ с помощью меньшей матрицы $P_{\theta}'$ и нейронной сети $MLP_{\theta}$\footnote{имхо это похоже на то, что мы backbone модели оставили как есть, а решатель задачи запихали из переда в зад модели}.
    \end{block}

\end{frame}


\begin{frame}{Эксперименты}
    
    \begin{block}{Датасеты}
        Для table-to-text задачи использовались датасеты E2E, WebNLG и DART, а для summarization использовался XSUM.
    \end{block}
    
    \begin{block}{Архитектуры}
        Для table-to-text использовались $\text{GPT-2}_{\text{MEDIUM}}$ и $\text{GPT-2}_{\text{LARGE}}$. Для summarization использовался $\text{BART}_{\text{LARGE}}$.
    \end{block}

\end{frame}

\begin{frame}{Результаты}

    \begin{block}{Table-to-text}
        Как видно по таблице~\ref{prefix:table-to-text}, prefix-tuning обходит другие бейзлайны\footnote{Adapter и FT-TOP2} добавляя всего лишь $0.1\%$ параметров и имеет качество, сравнимое с полным fine-tuning на всех трех датасетах. Если уменьшить количество параметров для метода adapter до тех же $0.1\%$, его качество станет сильно хуже prefix-tuning.
    \end{block}

    \begin{block}{Summarization}
        Судя по таблице~\ref{prefix:summarization}, prefix-tuning уже уступает полному fine-tuning в силу более сложного датасета.
    \end{block}

\end{frame}

\begin{frame}{Таблица}

    \begin{figure}
        \label{prefix:table-to-text}
        \caption{Table-to-text}
        \begin{center}
            \includegraphics[scale=0.27]{images/prefix_2.jpg}
        \end{center}
    \end{figure}

    \begin{figure}
        \label{prefix:summarization}
        \caption{Summarization}
        \begin{center}
            \includegraphics[scale=0.3]{images/prefix_3.jpg}
        \end{center}
    \end{figure}

\end{frame}

\section{LoRA}

\begin{frame}{LoRA}
    
    Последняя рассматриваемая техника PEFT это LoRA~\cite{lora}, то есть Low-Rank Adaptation of Large Language Models. Для начала вспомним одно из определений ранга матрицы:

    \begin{definition}[Ранг матрицы]
        Рангом матрицы $A \in \mathbb{R}^{m \times n}$ будем называть минимальное неотрицательное целое число $k$ такое, что $A$ можно представить в виде произведения $A = C R$ двух матриц\footnote{картинку я рисовал честно в техе} $C \in \mathbb{R}^{m \times k}$ и $R \in \mathbb{R}^{k \times n}$.

        \begin{figure}
            \begin{center}
                \begin{tikzpicture}
                    \node[left] at (0, 1.5) {$m$};
                    \node[above] at (1, 3) {$n$};
                    \filldraw[fill=blue!20!white, draw=black] (0, 0) rectangle (2, 3) node[pos=.5] {$A$};
                    \node at (2.5, 1.5) {$=$};
                    \node[left] at (3.2, 1.5) {$m$};
                    \node[above] at (3.7, 3) {$k$};
                    \filldraw[fill=pink!20!white, draw=black] (3.2, 0) rectangle (4.2, 3) node[pos=.5] {$C$};
                    \node at (4.45, 1.5) {$\times$};
                    \node[left] at (4.9, 1.5) {$k$};
                    \node[above] at (5.9, 2) {$n$};
                    \filldraw[fill=yellow!20!white, draw=black] (4.9, 1) rectangle (6.9, 2) node[pos=.5] {$R$};
                \end{tikzpicture}
            \end{center}
        \end{figure}

    \end{definition}

\end{frame}


\begin{frame}[allowframebreaks]
    \frametitle{Список литературы}
    \printbibliography
\end{frame}


\end{document}